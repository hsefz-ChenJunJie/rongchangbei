### **阶段一：框架与API定义 (目标：生成API文档，解放前端)**

**指令 1.1：项目初始化**

> 请你使用 mamba 为项目新建一个虚拟环境，使用Python和FastAPI创建一个新的Web后端项目。初始化项目结构，配置CORS允许所有源。创建一个根路由 `/` 作为健康检查点。

**指令 1.2：定义STT接口**

> 在项目中添加一个用于语音转文字(STT)的API端点 `/api/stt`。它应该接收音频文件作为输入，并返回包含转写文本的JSON。现在只需定义好输入输出的数据模型和空的路由函数，确保它能出现在API文档中。

**指令 1.3：定义TTS接口**

> 接下来，添加一个文字转语音(TTS)的API端点 `/api/tts`。它接收包含文本的JSON，并返回音频流。同样，现在只需定义好数据模型和空的路由函数。

**指令 1.4：定义LLM核心接口**

> 最后，添加核心的回答建议生成接口 `/api/generate_suggestions`。为它定义一个复杂的Pydantic输入模型，包含`scenario_context`, `user_opinion`, `target_dialogue`, `modification_suggestion`, `suggestion_count`等字段。其输出应为一个包含建议列表的JSON。完成数据模型和空函数的定义。

**指令 1.5：生成API文档**

> 在完成这些内容后，请你确保运行 FastAPI 后能正常生成 API 文档供前端使用。

---

### **阶段二：核心逻辑实现 (目标：填充功能)**

**指令 2.1：实现STT功能**

> 现在我们来完成 `/api/stt` 端点的功能。
>
> 在这个路由函数内部，你需要实现以下逻辑：
> 1.  接收上传的音频文件。
> 2.  **[在此处调用STT模型]** 对该音频文件进行处理。
> 3.  将模型返回的文本封装到预定义的JSON响应体中并返回。
>
> *(稍后我会告诉你如何调用STT模型。)*

**指令 2.2：实现TTS功能**

> 接下来完成 `/api/tts` 端点的功能。
>
> 函数逻辑如下：
> 1.  接收包含文本的JSON输入。
> 2.  **[在此处调用TTS模型]** 使用接收到的文本生成语音。
> 3.  将模型生成的音频数据作为文件流直接返回。
>
> *(稍后我会告诉你如何调用TTS模型。)*

**指令 2.3：实现LLM核心功能**

> 现在，我们来实现最关键的 `/api/generate_suggestions` 端点。
>
> 函数逻辑如下：
> 1.  接收包含各种上下文信息的JSON输入。
> 2.  根据所有输入的有效字段，动态地、智能地组合成一个结构化的Prompt。
> 3.  **[在此处调用LLM模型]** 并将构建好的Prompt作为输入。
> 4.  解析LLM返回的结果，提取出建议列表，并将其封装到预定义的JSON响应体中返回。
>
> *(稍后我会告诉你如何调用LLM模型以及如何构建Prompt。)*

---