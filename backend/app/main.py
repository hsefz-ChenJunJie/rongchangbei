from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from sse_starlette.sse import EventSourceResponse
from pydantic import BaseModel
from typing import List, Optional
import io
import os
import tempfile
import time
import logging
import json
import asyncio
import aiohttp
import traceback

# é…ç½®æ—¥å¿—
# è®¾ç½®ä¸º DEBUG çº§åˆ«ä»¥æŸ¥çœ‹è¯¦ç»†è°ƒè¯•ä¿¡æ¯ï¼Œè®¾ç½®ä¸º INFO ä»¥æŸ¥çœ‹ä¸€èˆ¬ä¿¡æ¯
LOG_LEVEL = logging.DEBUG  # å¯ä»¥æ”¹ä¸º logging.INFO æ¥å‡å°‘æ—¥å¿—è¾“å‡º

logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ä¸ºäº†é¿å…aiohttpçš„è°ƒè¯•æ—¥å¿—è¿‡å¤šï¼Œå•ç‹¬è®¾ç½®å…¶æ—¥å¿—çº§åˆ«
logging.getLogger('aiohttp').setLevel(logging.WARNING)
logging.getLogger('asyncio').setLevel(logging.WARNING)

# ==================== è¿œç¨‹APIæœåŠ¡å•†é…ç½® ====================
# è¯·åœ¨è¿™é‡Œå¡«å†™æ‚¨çš„è¿œç¨‹APIæœåŠ¡å•†ä¿¡æ¯
REMOTE_API_CONFIG = {
    # ç¤ºä¾‹é…ç½® - è¯·æ ¹æ®æ‚¨çš„æœåŠ¡å•†ä¿®æ”¹ä»¥ä¸‹ä¿¡æ¯
    "api_url": "https://openrouter.ai/api/v1",  # æ›¿æ¢ä¸ºæ‚¨çš„APIåœ°å€
    "api_key": "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",  # æ›¿æ¢ä¸ºæ‚¨çš„APIå¯†é’¥
    "model_name": "qwen/qwen3-32b:free",  # æ›¿æ¢ä¸ºæ‚¨è¦ä½¿ç”¨çš„æ¨¡å‹åç§°
    "temperature": 0.7,
    "top_p": 0.9,
    "stream": True,
    
    # å¸¸è§æœåŠ¡å•†é¢„è®¾é…ç½®ï¼ˆå–æ¶ˆæ³¨é‡Šå¹¶å¡«å†™å¯¹åº”çš„API Keyï¼‰
    # OpenAIé…ç½®
    # "api_url": "https://api.openai.com/v1/chat/completions",
    # "api_key": "sk-your-openai-key",
    # "model_name": "gpt-3.5-turbo",
    
    # æ™ºè°±AIé…ç½®
    # "api_url": "https://open.bigmodel.cn/api/paas/v4/chat/completions",
    # "api_key": "your-zhipu-key",
    # "model_name": "glm-4",
    
    # ç™¾åº¦åƒå¸†é…ç½®
    # "api_url": "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions",
    # "api_key": "your-baidu-key",
    # "model_name": "ernie-bot-turbo",
    
    # é˜¿é‡Œäº‘é€šä¹‰åƒé—®é…ç½®
    # "api_url": "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
    # "api_key": "your-aliyun-key",
    # "model_name": "qwen-turbo",
}

# å…¨å±€æ¨¡å‹å®ä¾‹
stt_model = None
stt_processor = None
stt_tokenizer = None
tts_model = None
llm_model = None
use_remote_llm = False  # æ ‡è®°æ˜¯å¦ä½¿ç”¨è¿œç¨‹LLM

def load_stt_model():
    """
    åŠ è½½STTæ¨¡å‹ï¼Œåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨
    æ”¯æŒtransformersæ ¼å¼å’ŒåŸç”Ÿwhisperæ ¼å¼
    """
    global stt_model, stt_processor, stt_tokenizer
    try:
        # æ¨¡å‹è·¯å¾„é…ç½®
        model_path = os.path.join(os.path.dirname(__file__), "..", "models", "stt")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨transformersæ ¼å¼çš„æ¨¡å‹
        if os.path.exists(model_path) and os.path.isfile(os.path.join(model_path, "config.json")):
            try:
                # å°è¯•åŠ è½½transformersæ ¼å¼çš„whisperæ¨¡å‹
                from transformers import WhisperProcessor, WhisperForConditionalGeneration
                
                logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ°transformersæ ¼å¼çš„STTæ¨¡å‹: {model_path}")
                stt_processor = WhisperProcessor.from_pretrained(model_path)
                stt_model = WhisperForConditionalGeneration.from_pretrained(model_path)
                stt_tokenizer = stt_processor.tokenizer
                
                logger.info("Transformersæ ¼å¼STTæ¨¡å‹åŠ è½½æˆåŠŸ")
                return True
                
            except Exception as e:
                logger.warning(f"Transformersæ ¼å¼æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}, å°è¯•åŸç”Ÿwhisperæ ¼å¼")
        
        # å›é€€åˆ°åŸç”Ÿwhisperæ ¼å¼
        import whisper
        
        # å°è¯•åŠ è½½æœ¬åœ°whisperæ¨¡å‹æ–‡ä»¶
        local_model_files = []
        if os.path.exists(model_path):
            for file in os.listdir(model_path):
                if file.endswith(('.pt', '.pth', '.bin')):
                    local_model_files.append(os.path.join(model_path, file))
        
        if local_model_files:
            # åŠ è½½æœ¬åœ°whisperæ¨¡å‹
            model_file = local_model_files[0]
            logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ°whisperæ ¼å¼STTæ¨¡å‹: {model_file}")
            stt_model = whisper.load_model(model_file)
        else:
            # å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨whisperé»˜è®¤æ¨¡å‹
            logger.info("æœªæ‰¾åˆ°æœ¬åœ°STTæ¨¡å‹ï¼Œä½¿ç”¨whisperé»˜è®¤baseæ¨¡å‹")
            stt_model = whisper.load_model("base")
            
        logger.info("åŸç”Ÿwhisperæ ¼å¼STTæ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
        
    except ImportError as e:
        logger.error(f"å¿…è¦çš„åº“æœªå®‰è£…: {str(e)}")
        return False
    except Exception as e:
        logger.error(f"STTæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return False


def load_tts_model():
    """
    åŠ è½½TTSæ¨¡å‹ï¼Œåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨
    ä½¿ç”¨Coqui TTSåº“ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
    """
    global tts_model
    try:
        # å¯¼å…¥TTSåº“
        from TTS.api import TTS
        
        # ç›´æ¥ä½¿ç”¨æ¨¡å‹åç§°åŠ è½½TTSæ¨¡å‹
        logger.info("ä½¿ç”¨æ¨¡å‹åç§°åŠ è½½TTSæ¨¡å‹")
        
        try:
            # ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡TTSæ¨¡å‹
            logger.info("æ­£åœ¨åŠ è½½ä¸­æ–‡TTSæ¨¡å‹...")
            tts_model = TTS(model_name="tts_models/zh-CN/baker/vits")
            logger.info("TTSæ¨¡å‹åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨ä¸­æ–‡æ¨¡å‹ï¼‰")
        except Exception as e:
            logger.warning(f"ä¸­æ–‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€åˆ°è‹±æ–‡æ¨¡å‹
            logger.info("å°è¯•ä½¿ç”¨è‹±æ–‡TTSæ¨¡å‹")
            tts_model = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC")
            logger.info("TTSæ¨¡å‹åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨è‹±æ–‡æ¨¡å‹ï¼‰")
            
        logger.info("TTSæ¨¡å‹åŠ è½½æˆåŠŸ")
        return True
        
    except ImportError:
        logger.error("TTSåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install TTS")
        return False
    except Exception as e:
        logger.error(f"TTSæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return False


async def call_remote_llm_api(system_prompt: str, user_prompt: str):
    """
    è°ƒç”¨è¿œç¨‹APIæœåŠ¡å•†çš„LLM API
    è¿”å›æµå¼å“åº”ç”Ÿæˆå™¨
    """
    try:
        # æ„å»ºè¯·æ±‚æ•°æ®
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        request_data = {
            "model": REMOTE_API_CONFIG["model_name"],
            "messages": messages,
            "temperature": REMOTE_API_CONFIG["temperature"],
            "top_p": REMOTE_API_CONFIG["top_p"],
            "stream": REMOTE_API_CONFIG["stream"]
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {REMOTE_API_CONFIG['api_key']}"
        }
        
        logger.info(f"ğŸ“¡ è°ƒç”¨è¿œç¨‹API: {REMOTE_API_CONFIG['api_url']}")
        logger.info(f"ğŸ“ ä½¿ç”¨æ¨¡å‹: {REMOTE_API_CONFIG['model_name']}")
        logger.info(f"ğŸ’¬ ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
        logger.info(f"ğŸ’¬ ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)} å­—ç¬¦")
        logger.debug(f"ğŸ“‹ è¯·æ±‚æ•°æ®: {json.dumps(request_data, ensure_ascii=False, indent=2)}")
        
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=300)) as session:
            logger.info("ğŸ”„ å‘é€HTTPè¯·æ±‚...")
            async with session.post(
                REMOTE_API_CONFIG["api_url"], 
                json=request_data, 
                headers=headers
            ) as response:
                
                logger.info(f"ğŸ“¨ æ”¶åˆ°å“åº”: HTTP {response.status}")
                
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"âŒ è¿œç¨‹APIè°ƒç”¨å¤±è´¥: {response.status}")
                    logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {error_text}")
                    logger.error(f"âŒ å“åº”å¤´: {dict(response.headers)}")
                    raise Exception(f"è¿œç¨‹APIè°ƒç”¨å¤±è´¥: {response.status} - {error_text}")
                
                # å¤„ç†æµå¼å“åº”
                logger.info("ğŸ”„ å¼€å§‹å¤„ç†æµå¼å“åº”...")
                accumulated_text = ""
                token_count = 0
                
                async for line in response.content:
                    line_text = line.decode('utf-8').strip()
                    
                    # è·³è¿‡ç©ºè¡Œå’Œéæ•°æ®è¡Œ
                    if not line_text or not line_text.startswith('data: '):
                        continue
                    
                    # è§£æSSEæ•°æ®
                    data_text = line_text[6:]  # å»æ‰ "data: " å‰ç¼€
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
                    if data_text == '[DONE]':
                        logger.info("âœ… æµå¼å“åº”å®Œæˆ")
                        break
                    
                    try:
                        data = json.loads(data_text)
                        
                        # æå–tokenå†…å®¹
                        choices = data.get('choices', [])
                        if choices and 'delta' in choices[0]:
                            delta = choices[0]['delta']
                            if 'content' in delta:
                                token = delta['content']
                                accumulated_text += token
                                token_count += 1
                                
                                # æ¯100ä¸ªtokenè®°å½•ä¸€æ¬¡è¿›åº¦
                                if token_count % 100 == 0:
                                    logger.debug(f"ğŸ“Š å·²æ¥æ”¶ {token_count} ä¸ªtokenï¼Œå½“å‰é•¿åº¦: {len(accumulated_text)}")
                                
                                # ç”Ÿæˆtokenäº‹ä»¶
                                yield {
                                    "event": "token",
                                    "data": json.dumps({
                                        "token": token,
                                        "accumulated": accumulated_text
                                    }, ensure_ascii=False)
                                }
                                
                                # æ·»åŠ å°å»¶è¿Ÿ
                                await asyncio.sleep(0.01)
                                
                    except json.JSONDecodeError as e:
                        logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {e}, åŸå§‹æ•°æ®: {data_text[:100]}...")
                        continue
                
                logger.info(f"ğŸ“Š è¿œç¨‹APIè°ƒç”¨å®Œæˆï¼Œæ€»å…±æ¥æ”¶ {token_count} ä¸ªtokenï¼Œæœ€ç»ˆé•¿åº¦: {len(accumulated_text)}")
                
    except asyncio.TimeoutError:
        logger.error("âŒ è¿œç¨‹APIè°ƒç”¨è¶…æ—¶")
        raise Exception("è¿œç¨‹APIè°ƒç”¨è¶…æ—¶")
    except aiohttp.ClientError as e:
        logger.error(f"âŒ ç½‘ç»œè¿æ¥é”™è¯¯: {type(e).__name__}: {str(e)}")
        logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise Exception(f"ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}")
    except Exception as e:
        logger.error(f"âŒ è¿œç¨‹APIè°ƒç”¨å¤±è´¥: {type(e).__name__}: {str(e)}")
        logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise


def validate_remote_api_config():
    """
    éªŒè¯è¿œç¨‹APIé…ç½®æ˜¯å¦å®Œæ•´
    """
    required_fields = ["api_url", "api_key", "model_name"]
    
    for field in required_fields:
        if not REMOTE_API_CONFIG.get(field) or REMOTE_API_CONFIG[field] in [
            "https://api.your-provider.com/v1/chat/completions",
            "your-api-key-here",
            "your-model-name"
        ]:
            return False, f"è¯·é…ç½®è¿œç¨‹APIå‚æ•°: {field}"
    
    return True, "é…ç½®æœ‰æ•ˆ"


def load_llm_model():
    """
    åŠ è½½LLMæ¨¡å‹ï¼Œåœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨
    ä½¿ç”¨ctransformersåº“ä»æœ¬åœ°è·¯å¾„åŠ è½½GGUFæ ¼å¼çš„æ¨¡å‹
    å¦‚æœæœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨è¿œç¨‹APIæœåŠ¡å•†
    """
    global llm_model, use_remote_llm
    
    # é¦–å…ˆå°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹
    try:
        # å¯¼å…¥ctransformers
        from ctransformers import AutoModelForCausalLM
        
        # LLMæ¨¡å‹è·¯å¾„é…ç½®
        llm_model_path = os.path.join(os.path.dirname(__file__), "..", "models", "llm", "Qwen3-4B-Q4_0.gguf")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(llm_model_path):
            logger.warning(f"LLMæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {llm_model_path}")
            raise FileNotFoundError("æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(llm_model_path) / (1024**3)  # GB
        logger.info(f"LLMæ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.2f}GB")
        
        logger.info(f"æ­£åœ¨åŠ è½½LLMæ¨¡å‹: {llm_model_path}")
        
        # æ™ºèƒ½é…ç½®ç¡¬ä»¶å‚æ•°
        import psutil
        
        # è·å–ç³»ç»Ÿä¿¡æ¯
        cpu_count = psutil.cpu_count(logical=False)  # ç‰©ç†æ ¸å¿ƒæ•°
        memory_gb = psutil.virtual_memory().total / (1024**3)  # æ€»å†…å­˜GB
        
        # æ™ºèƒ½é…ç½®å‚æ•°
        context_length = 4096  # ä¸Šä¸‹æ–‡çª—å£å¤§å°
        threads = min(cpu_count, 8)  # çº¿ç¨‹æ•°ä¸è¶…è¿‡8
        
        # GPUé…ç½®ï¼ˆctransformersæ”¯æŒGPUåŠ é€Ÿï¼‰
        gpu = False
        try:
            import torch
            if torch.cuda.is_available():
                gpu = True
                logger.info("æ£€æµ‹åˆ°GPUï¼Œå¯ç”¨GPUåŠ é€Ÿ")
        except ImportError:
            logger.info("æœªæ£€æµ‹åˆ°PyTorchæˆ–CUDAï¼Œä½¿ç”¨CPUæ¨¡å¼")
        
        # æ ¹æ®å†…å­˜è°ƒæ•´ä¸Šä¸‹æ–‡çª—å£
        if memory_gb < 8:
            context_length = 2048  # å†…å­˜ä¸è¶³8GBæ—¶å‡å°‘ä¸Šä¸‹æ–‡çª—å£
            logger.info(f"å†…å­˜è¾ƒå°({memory_gb:.1f}GB)ï¼Œè°ƒæ•´ä¸Šä¸‹æ–‡çª—å£ä¸º{context_length}")
        
        logger.info(f"ç¡¬ä»¶é…ç½®: CPUæ ¸å¿ƒ={cpu_count}, å†…å­˜={memory_gb:.1f}GB, çº¿ç¨‹={threads}, ä¸Šä¸‹æ–‡={context_length}, GPU={gpu}")
        
        # åŠ è½½æ¨¡å‹
        logger.info("ğŸ”„ å¼€å§‹åŠ è½½æœ¬åœ°LLMæ¨¡å‹...")
        model_loaded = False
        
        try:
            logger.info("ğŸ“‹ å°è¯•æ–¹æ³•1ï¼šä½¿ç”¨æ ‡å‡†å‚æ•°åŠ è½½æ¨¡å‹")
            llm_model = AutoModelForCausalLM.from_pretrained(
                llm_model_path,
                model_type="qwen",  # æŒ‡å®šæ¨¡å‹ç±»å‹
                context_length=context_length,
                threads=threads,
                gpu_layers=-1 if gpu else 0,  # -1è¡¨ç¤ºå…¨éƒ¨å±‚ä½¿ç”¨GPUï¼Œ0è¡¨ç¤ºCPU
                stream=True,  # å¯ç”¨æµå¼è¾“å‡º
                local_files_only=True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            model_loaded = True
            logger.info("âœ… æ–¹æ³•1æˆåŠŸï¼šæ ‡å‡†å‚æ•°åŠ è½½å®Œæˆ")
            
        except Exception as e:
            error_msg = str(e)
            logger.warning(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {error_msg}")
            
            # å°è¯•ä½¿ç”¨æ›´ä¿å®ˆçš„å‚æ•°
            logger.info("ğŸ“‹ å°è¯•æ–¹æ³•2ï¼šä½¿ç”¨ä¿å®ˆå‚æ•°é‡æ–°åŠ è½½LLMæ¨¡å‹")
            try:
                llm_model = AutoModelForCausalLM.from_pretrained(
                    llm_model_path,
                    model_type="qwen",
                    context_length=2048,  # å‡å°‘ä¸Šä¸‹æ–‡çª—å£
                    threads=2,  # å‡å°‘çº¿ç¨‹æ•°
                    gpu_layers=0,  # ç¦ç”¨GPU
                    stream=True,
                    local_files_only=True
                )
                model_loaded = True
                logger.info("âœ… æ–¹æ³•2æˆåŠŸï¼šä¿å®ˆå‚æ•°åŠ è½½å®Œæˆ")
                
            except Exception as e2:
                logger.warning(f"âš ï¸ æ–¹æ³•2å¤±è´¥: {str(e2)}")
                
                # å°è¯•è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹
                logger.info("ğŸ“‹ å°è¯•æ–¹æ³•3ï¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹")
                try:
                    llm_model = AutoModelForCausalLM.from_pretrained(
                        llm_model_path,
                        context_length=2048,
                        threads=2,
                        gpu_layers=0,
                        stream=True,
                        local_files_only=True
                    )
                    model_loaded = True
                    logger.info("âœ… æ–¹æ³•3æˆåŠŸï¼šè‡ªåŠ¨æ£€æµ‹åŠ è½½å®Œæˆ")
                    
                except Exception as e3:
                    logger.error(f"âŒ æ–¹æ³•3å¤±è´¥: {str(e3)}")
                    logger.error("âŒ æ‰€æœ‰æœ¬åœ°æ¨¡å‹åŠ è½½æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°†åˆ‡æ¢åˆ°è¿œç¨‹API")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½
        if model_loaded and llm_model is not None:
            logger.info("âœ… æœ¬åœ°LLMæ¨¡å‹åŠ è½½æˆåŠŸ")
            use_remote_llm = False
            return True
        else:
            logger.error("âŒ æœ¬åœ°LLMæ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥")
            # æŠ›å‡ºå¼‚å¸¸ä»¥è§¦å‘è¿œç¨‹APIå›é€€
            raise Exception("æ‰€æœ‰æœ¬åœ°æ¨¡å‹åŠ è½½æ–¹æ³•éƒ½å¤±è´¥")
        
    except ImportError:
        logger.warning("ctransformersåº“æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨è¿œç¨‹API")
    except Exception as e:
        logger.warning(f"æœ¬åœ°LLMæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}ï¼Œå°è¯•ä½¿ç”¨è¿œç¨‹API")
    
    # æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨è¿œç¨‹API
    try:
        logger.info("ğŸ”„ å°è¯•é…ç½®è¿œç¨‹APIä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ...")
        
        # éªŒè¯è¿œç¨‹APIé…ç½®
        is_valid, message = validate_remote_api_config()
        if not is_valid:
            logger.error(f"âŒ è¿œç¨‹APIé…ç½®æ— æ•ˆ: {message}")
            logger.error("âŒ è¯·åœ¨ä»£ç é¡¶éƒ¨çš„REMOTE_API_CONFIGä¸­é…ç½®æ‚¨çš„APIæœåŠ¡å•†ä¿¡æ¯")
            logger.error("âŒ éœ€è¦é…ç½®çš„å­—æ®µ: api_url, api_key, model_name")
            return False
        
        # æµ‹è¯•è¿œç¨‹APIè¿æ¥
        logger.info("ğŸ”„ æ­£åœ¨éªŒè¯è¿œç¨‹APIé…ç½®...")
        
        # è¿™é‡Œä¸è¿›è¡Œå®é™…çš„APIè°ƒç”¨æµ‹è¯•ï¼Œåªæ˜¯æ ‡è®°ä½¿ç”¨è¿œç¨‹API
        use_remote_llm = True
        llm_model = None  # è¿œç¨‹APIä¸éœ€è¦æœ¬åœ°æ¨¡å‹å®ä¾‹
        
        logger.info(f"âœ… å·²é…ç½®è¿œç¨‹API: {REMOTE_API_CONFIG['api_url']}")
        logger.info(f"âœ… ä½¿ç”¨æ¨¡å‹: {REMOTE_API_CONFIG['model_name']}")
        logger.info(f"âœ… æ¸©åº¦è®¾ç½®: {REMOTE_API_CONFIG['temperature']}")
        logger.info(f"âœ… Top-pè®¾ç½®: {REMOTE_API_CONFIG['top_p']}")
        logger.info("âœ… è¿œç¨‹LLM APIé…ç½®æˆåŠŸ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è¿œç¨‹APIé…ç½®å¤±è´¥: {type(e).__name__}: {str(e)}")
        logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return False


def load_system_prompt() -> str:
    """
    ä»llm.mdæ–‡ä»¶åŠ è½½ç³»ç»Ÿæç¤ºè¯
    """
    try:
        system_prompt_path = os.path.join(os.path.dirname(__file__), "..", "llm.md")
        with open(system_prompt_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except Exception as e:
        logger.error(f"åŠ è½½ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {e}")
        # å›é€€åˆ°é»˜è®¤ç³»ç»Ÿæç¤ºè¯
        return "ä½ æ˜¯ä¸€ä¸ªé¡¶çº§çš„ã€å¯Œæœ‰åŒæƒ…å¿ƒå’Œé«˜æƒ…å•†çš„æ²Ÿé€šåŠ©æ‰‹ï¼Œä¸“é—¨ä¸ºä¸€ä¸ªæœ‰è¯­è¨€éšœç¢çš„ç”¨æˆ·æä¾›æ”¯æŒã€‚\nä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ï¼šåœ¨ç”¨æˆ·éœ€è¦æ—¶ï¼Œæ ¹æ®å¯¹æ–¹çš„è°ˆè¯å†…å®¹å’Œç”¨æˆ·çš„åŸºæœ¬æ„å›¾ï¼Œä»å¤šç§è§’åº¦è€ƒè™‘é—®é¢˜ï¼Œç”Ÿæˆå¤šç§é£æ ¼çš„ã€é«˜è´¨é‡çš„å›ç­”å»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·æµç•…ã€è‡ªä¿¡åœ°è¿›è¡Œäº¤æµã€‚\n**ä½ çš„è¡Œä¸ºå‡†åˆ™:**\n1.  **å¤šæ ·æ€§**: æ°¸è¿œæä¾›å¤šç§ä¸åŒè¯­æ°”å’Œé£æ ¼çš„å»ºè®®ï¼ˆä¾‹å¦‚ï¼šç®€æ´ç›´æ¥ã€ç¤¼è²Œå§”å©‰ã€å¹½é»˜å‹å¥½ã€æå‡ºåé—®ç­‰ï¼‰ã€‚\n2.  **åŒç†å¿ƒ**: ä½ çš„å›ç­”å»ºè®®åº”å§‹ç»ˆä¿æŒç§¯æã€å°Šé‡å’Œæ”¯æŒçš„æ€åº¦ã€‚"


def build_structured_prompt(request: 'GenerateSuggestionsRequest') -> str:
    """
    æ ¹æ®è¯·æ±‚å†…å®¹æ„å»ºç»“æ„åŒ–çš„ç”¨æˆ·Prompt
    """
    prompt_parts = []
    
    # å¤„ç†å¯¹è¯æƒ…æ™¯
    if request.scenario_context and request.scenario_context.strip():
        prompt_parts.append(f"å¯¹è¯æƒ…æ™¯ï¼š{request.scenario_context.strip()}")
    
    # å¤„ç†ç›®æ ‡å¯¹è¯å†…å®¹
    if request.target_dialogue and request.target_dialogue.strip():
        prompt_parts.append(f"å¯¹è¯å†…å®¹ï¼š{request.target_dialogue.strip()}")
    
    # å¤„ç†ç”¨æˆ·æ„è§
    if request.user_opinion and request.user_opinion.strip():
        prompt_parts.append(f"ç”¨æˆ·æ„è§ï¼š{request.user_opinion.strip()}")
    
    # å¤„ç†ä¿®æ”¹å»ºè®®
    if request.modification_suggestion and len(request.modification_suggestion) > 0:
        modifications = [mod.strip() for mod in request.modification_suggestion if mod.strip()]
        if modifications:
            prompt_parts.append("ä¸¥æ ¼éµå¾ªä»¥ä¸‹ä¿®æ”¹æ„è§ï¼š")
            prompt_parts.append(' '.join(modifications))
    
    # ç”Ÿæˆè¦æ±‚
    suggestion_count = request.suggestion_count or 3
    prompt_parts.append(f"è¯·ç”Ÿæˆ{suggestion_count}æ¡é«˜è´¨é‡çš„å›ç­”å»ºè®®ã€‚")
    
    # è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå› ä¸ºctransformersä¸æ”¯æŒJSON Schemaï¼Œéœ€è¦åœ¨æç¤ºè¯ä¸­æŒ‡å®šï¼‰
    prompt_parts.append("è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„æ–‡å­—è¯´æ˜ï¼š")
    prompt_parts.append(json.dumps({
        "suggestions": [
            {
                "id": 1,
                "content": "å»ºè®®å†…å®¹1",
                "confidence": 0.85
            },
            {
                "id": 2,
                "content": "å»ºè®®å†…å®¹2",
                "confidence": 0.80
            }
        ]
    }, ensure_ascii=False, indent=2))
    
    return '\n\n'.join(prompt_parts)


def get_json_schema() -> dict:
    """
    è·å–JSON Schemaç”¨äºå¼ºåˆ¶æ ¼å¼åŒ–è¾“å‡º
    """
    return {
        "type": "object",
        "properties": {
            "suggestions": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "integer"},
                        "content": {"type": "string"},
                        "confidence": {"type": "number", "minimum": 0, "maximum": 1}
                    },
                    "required": ["id", "content", "confidence"]
                }
            }
        },
        "required": ["suggestions"]
    }


def apply_qwen2_chat_template(system_prompt: str, user_prompt: str) -> str:
    """
    åº”ç”¨Qwen2æ¨¡å‹çš„èŠå¤©æ¨¡æ¿ï¼Œåˆ†ç¦»ç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯
    """
    # Qwen2çš„èŠå¤©æ¨¡æ¿æ ¼å¼
    chat_template = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_prompt}<|im_end|>
<|im_start|>assistant
"""
    
    return chat_template


app = FastAPI(title="è£æ˜¶æ¯é¡¹ç›® API", version="1.0.0")

# åº”ç”¨å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œçš„äº‹ä»¶"""
    logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨è£æ˜¶æ¯é¡¹ç›® API...")
    logger.info("=" * 50)
    
    # åŠ è½½STTæ¨¡å‹
    logger.info("ğŸ”„ å¼€å§‹åŠ è½½STTæ¨¡å‹...")
    stt_success = load_stt_model()
    if not stt_success:
        logger.warning("âš ï¸ STTæ¨¡å‹åŠ è½½å¤±è´¥ï¼ŒSTTåŠŸèƒ½å°†ä¸å¯ç”¨")
    else:
        logger.info("âœ… STTæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åŠ è½½TTSæ¨¡å‹
    logger.info("ğŸ”„ å¼€å§‹åŠ è½½TTSæ¨¡å‹...")
    tts_success = load_tts_model()
    if not tts_success:
        logger.warning("âš ï¸ TTSæ¨¡å‹åŠ è½½å¤±è´¥ï¼ŒTTSåŠŸèƒ½å°†ä¸å¯ç”¨")
    else:
        logger.info("âœ… TTSæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åŠ è½½LLMæ¨¡å‹
    logger.info("ğŸ”„ å¼€å§‹åŠ è½½LLMæ¨¡å‹...")
    llm_success = load_llm_model()
    if not llm_success:
        logger.warning("âš ï¸ LLMæ¨¡å‹åŠ è½½å¤±è´¥ï¼ŒLLMåŠŸèƒ½å°†ä¸å¯ç”¨")
    else:
        logger.info("âœ… LLMæ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # å¯åŠ¨æ€»ç»“
    logger.info("=" * 50)
    logger.info("ğŸ“Š å¯åŠ¨çŠ¶æ€æ€»ç»“:")
    logger.info(f"   STT: {'âœ… å¯ç”¨' if stt_success else 'âŒ ä¸å¯ç”¨'}")
    logger.info(f"   TTS: {'âœ… å¯ç”¨' if tts_success else 'âŒ ä¸å¯ç”¨'}")
    logger.info(f"   LLM: {'âœ… å¯ç”¨' if llm_success else 'âŒ ä¸å¯ç”¨'}")
    logger.info(f"   æœåŠ¡æ¨¡å¼: {'ğŸŒ è¿œç¨‹API' if use_remote_llm else 'ğŸ  æœ¬åœ°æ¨¡å‹'}")
    logger.info("ğŸ‰ è£æ˜¶æ¯é¡¹ç›® API å¯åŠ¨å®Œæˆ!")
    logger.info("=" * 50)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def health_check():
    return {"status": "healthy", "message": "è£æ˜¶æ¯é¡¹ç›® API is running"}


# STT æ•°æ®æ¨¡å‹
class STTResponse(BaseModel):
    text: str
    confidence: Optional[float] = None
    processing_time: Optional[float] = None


# TTS æ•°æ®æ¨¡å‹
class TTSRequest(BaseModel):
    text: str
    voice: Optional[str] = "default"
    speed: Optional[float] = 1.0
    speaker: Optional[str] = None  # å¤šè¯è€…æ¨¡å‹éœ€è¦çš„è¯´è¯äººå‚æ•°


# LLM æ•°æ®æ¨¡å‹
class GenerateSuggestionsRequest(BaseModel):
    scenario_context: Optional[str] = None
    user_opinion: Optional[str] = None
    target_dialogue: Optional[str] = None
    modification_suggestion: Optional[List[str]] = None
    suggestion_count: Optional[int] = 3


class Suggestion(BaseModel):
    id: int
    content: str
    confidence: Optional[float] = None


class GenerateSuggestionsResponse(BaseModel):
    suggestions: List[Suggestion]
    processing_time: Optional[float] = None


# API ç«¯ç‚¹å®šä¹‰
@app.post("/api/stt", response_model=STTResponse)
async def speech_to_text(audio: UploadFile = File(...)):
    """
    è¯­éŸ³è½¬æ–‡å­— API
    
    æ¥æ”¶éŸ³é¢‘æ–‡ä»¶å¹¶è¿”å›è½¬å†™çš„æ–‡æœ¬
    æ”¯æŒtransformersæ ¼å¼å’ŒåŸç”Ÿwhisperæ ¼å¼çš„æ¨¡å‹
    """
    global stt_model, stt_processor, stt_tokenizer
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    if stt_model is None:
        raise HTTPException(status_code=503, detail="STTæ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡ä¸å¯ç”¨")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    temp_audio_path = None
    
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not audio.content_type or not any(mime in audio.content_type for mime in ['audio/', 'video/']):
            raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶")
        
        # è¯»å–ä¸Šä¼ çš„éŸ³é¢‘æ•°æ®
        audio_data = await audio.read()
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜éŸ³é¢‘
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
            temp_audio_path = temp_file.name
            temp_file.write(audio_data)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
        if stt_processor is not None:
            # ä½¿ç”¨transformersæ ¼å¼çš„æ¨¡å‹
            transcribed_text, confidence = await transcribe_with_transformers(temp_audio_path)
        else:
            # ä½¿ç”¨åŸç”Ÿwhisperæ ¼å¼çš„æ¨¡å‹
            transcribed_text, confidence = await transcribe_with_whisper(temp_audio_path)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = time.time() - start_time
        
        logger.info(f"STTå¤„ç†å®Œæˆ: {transcribed_text[:50]}... (è€—æ—¶: {processing_time:.2f}s)")
        
        return STTResponse(
            text=transcribed_text,
            confidence=confidence,
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"STTå¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_audio_path and os.path.exists(temp_audio_path):
            try:
                os.unlink(temp_audio_path)
                logger.debug(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_audio_path}")
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")


async def transcribe_with_transformers(audio_path: str):
    """
    ä½¿ç”¨transformersæ ¼å¼çš„whisperæ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
    """
    import torch
    import librosa
    
    # åŠ è½½éŸ³é¢‘æ–‡ä»¶
    audio_array, sampling_rate = librosa.load(audio_path, sr=16000)
    
    # é¢„å¤„ç†éŸ³é¢‘
    input_features = stt_processor(
        audio_array, 
        sampling_rate=sampling_rate, 
        return_tensors="pt"
    ).input_features
    
    # è®¾ç½®ä¸­æ–‡è¯­è¨€token
    forced_decoder_ids = stt_processor.get_decoder_prompt_ids(language="chinese", task="transcribe")
    
    # ç”Ÿæˆè½¬å†™ç»“æœ
    with torch.no_grad():
        predicted_ids = stt_model.generate(
            input_features,
            forced_decoder_ids=forced_decoder_ids,
            max_length=448,
            num_beams=5,
            early_stopping=True
        )
    
    # è§£ç ç»“æœ
    transcribed_text = stt_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    
    # ç®€å•çš„ç½®ä¿¡åº¦ä¼°ç®—ï¼ˆtransformersæ¨¡å‹æ²¡æœ‰ç›´æ¥çš„ç½®ä¿¡åº¦ï¼‰
    confidence = 0.85  # å›ºå®šå€¼ï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥åŸºäºæ¨¡å‹è¾“å‡ºè®¡ç®—
    
    return transcribed_text.strip(), confidence


async def transcribe_with_whisper(audio_path: str):
    """
    ä½¿ç”¨åŸç”Ÿwhisperæ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
    """
    # ä½¿ç”¨whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«
    result = stt_model.transcribe(
        audio_path,
        language="zh",  # æŒ‡å®šä¸­æ–‡
        task="transcribe"
    )
    
    # æå–è¯†åˆ«ç»“æœ
    transcribed_text = result.get("text", "").strip()
    
    # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºsegmentsè®¡ç®—å¹³å‡å€¼ï¼‰
    confidence = 0.0
    if "segments" in result and result["segments"]:
        confidences = []
        for segment in result["segments"]:
            if "avg_logprob" in segment:
                # å°†å¯¹æ•°æ¦‚ç‡è½¬æ¢ä¸ºç½®ä¿¡åº¦ï¼ˆè¿‘ä¼¼ï¼‰
                conf = min(1.0, max(0.0, (segment["avg_logprob"] + 1.0)))
                confidences.append(conf)
        if confidences:
            confidence = sum(confidences) / len(confidences)
    
    return transcribed_text, confidence


@app.get("/api/tts/speakers")
async def get_tts_speakers():
    """
    è·å–TTSæ¨¡å‹å¯ç”¨çš„è¯´è¯äººåˆ—è¡¨
    """
    global tts_model
    
    if tts_model is None:
        raise HTTPException(status_code=503, detail="TTSæ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡ä¸å¯ç”¨")
    
    try:
        speakers = []
        
        # æ£€æŸ¥ä¸åŒçš„è¯´è¯äººå±æ€§
        if hasattr(tts_model, 'speakers') and tts_model.speakers:
            speakers = list(tts_model.speakers)
        elif hasattr(tts_model, 'speaker_manager') and tts_model.speaker_manager:
            speakers = tts_model.speaker_manager.speaker_names
        
        return {
            "speakers": speakers,
            "default_speaker": speakers[0] if speakers else None,
            "is_multi_speaker": len(speakers) > 1
        }
    except Exception as e:
        logger.error(f"ğŸ“‹ è·å–è¯´è¯äººåˆ—è¡¨å¤±è´¥: {e}")
        return {
            "speakers": [],
            "default_speaker": None,
            "is_multi_speaker": False,
            "error": str(e)
        }


@app.post("/api/tts")
async def text_to_speech(request: TTSRequest):
    """
    æ–‡å­—è½¬è¯­éŸ³ API
    
    æ¥æ”¶æ–‡æœ¬å¹¶è¿”å›éŸ³é¢‘æµ
    ä½¿ç”¨Coqui TTSæ¨¡å‹è¿›è¡Œè¯­éŸ³åˆæˆ
    """
    global tts_model
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    if tts_model is None:
        raise HTTPException(status_code=503, detail="TTSæ¨¡å‹æœªåŠ è½½ï¼ŒæœåŠ¡ä¸å¯ç”¨")
    
    # éªŒè¯è¾“å…¥
    if not request.text or not request.text.strip():
        raise HTTPException(status_code=400, detail="è¯·æä¾›è¦åˆæˆçš„æ–‡æœ¬å†…å®¹")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    temp_audio_path = None
    
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜éŸ³é¢‘
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
            temp_audio_path = temp_file.name
        
        # ä½¿ç”¨TTSæ¨¡å‹è¿›è¡Œè¯­éŸ³åˆæˆ
        logger.info(f"ğŸ”Š æ­£åœ¨åˆæˆè¯­éŸ³: {request.text[:50]}...")
        
        # å‡†å¤‡TTSå‚æ•°
        tts_kwargs = {
            "text": request.text,
            "file_path": temp_audio_path,
            "speed": request.speed if request.speed else 1.0
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè¯è€…æ¨¡å‹å¹¶æ·»åŠ è¯´è¯äººå‚æ•°
        try:
            # å°è¯•è·å–æ¨¡å‹çš„è¯´è¯äººä¿¡æ¯
            if hasattr(tts_model, 'speakers') and tts_model.speakers:
                # å¤šè¯è€…æ¨¡å‹
                if request.speaker:
                    # ç”¨æˆ·æŒ‡å®šäº†è¯´è¯äºº
                    if request.speaker in tts_model.speakers:
                        tts_kwargs["speaker"] = request.speaker
                        logger.info(f"ğŸ¤ ä½¿ç”¨æŒ‡å®šè¯´è¯äºº: {request.speaker}")
                    else:
                        logger.warning(f"âš ï¸ æŒ‡å®šçš„è¯´è¯äºº '{request.speaker}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è¯´è¯äºº")
                        tts_kwargs["speaker"] = tts_model.speakers[0]
                else:
                    # ç”¨æˆ·æœªæŒ‡å®šè¯´è¯äººï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„è¯´è¯äºº
                    tts_kwargs["speaker"] = tts_model.speakers[0]
                    logger.info(f"ğŸ¤ ä½¿ç”¨é»˜è®¤è¯´è¯äºº: {tts_model.speakers[0]}")
            elif hasattr(tts_model, 'speaker_manager') and tts_model.speaker_manager:
                # å¦ä¸€ç§å¤šè¯è€…æ¨¡å‹ç»“æ„
                speakers = tts_model.speaker_manager.speaker_names
                if speakers:
                    if request.speaker and request.speaker in speakers:
                        tts_kwargs["speaker"] = request.speaker
                        logger.info(f"ğŸ¤ ä½¿ç”¨æŒ‡å®šè¯´è¯äºº: {request.speaker}")
                    else:
                        tts_kwargs["speaker"] = speakers[0]
                        logger.info(f"ğŸ¤ ä½¿ç”¨é»˜è®¤è¯´è¯äºº: {speakers[0]}")
        except Exception as e:
            logger.warning(f"âš ï¸ æ£€æŸ¥è¯´è¯äººä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        # è®°å½•æœ€ç»ˆçš„TTSå‚æ•°
        logger.debug(f"ğŸ“‹ TTSè°ƒç”¨å‚æ•°: {tts_kwargs}")
        
        # è°ƒç”¨TTSæ¨¡å‹ç”ŸæˆéŸ³é¢‘
        logger.info("ğŸ”„ å¼€å§‹TTSæ¨¡å‹éŸ³é¢‘ç”Ÿæˆ...")
        tts_model.tts_to_file(**tts_kwargs)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = time.time() - start_time
        logger.info(f"TTSå¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}s")
        
        # è¯»å–ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
        def generate_audio():
            try:
                with open(temp_audio_path, "rb") as audio_file:
                    while True:
                        chunk = audio_file.read(8192)  # 8KB chunks
                        if not chunk:
                            break
                        yield chunk
            except Exception as e:
                logger.error(f"è¯»å–éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
                yield b""
            finally:
                # åœ¨ç”Ÿæˆå™¨ç»“æŸæ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_audio_path and os.path.exists(temp_audio_path):
                    try:
                        os.unlink(temp_audio_path)
                        logger.debug(f"å·²æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶: {temp_audio_path}")
                    except Exception as e:
                        logger.warning(f"æ¸…ç†ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
        
        # è¿”å›éŸ³é¢‘æµ
        return StreamingResponse(
            generate_audio(),
            media_type="audio/wav",
            headers={
                "Content-Disposition": "attachment; filename=tts_output.wav",
                "X-Processing-Time": str(processing_time)
            }
        )
        
    except Exception as e:
        logger.error(f"TTSå¤„ç†å¤±è´¥: {str(e)}")
        # ç¡®ä¿åœ¨å‡ºé”™æ—¶ä¹Ÿæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_audio_path and os.path.exists(temp_audio_path):
            try:
                os.unlink(temp_audio_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")


@app.post("/api/generate_suggestions")
async def generate_suggestions(request: GenerateSuggestionsRequest):
    """
    ç”Ÿæˆå›ç­”å»ºè®® APIï¼ˆæµå¼å“åº”ï¼‰
    
    æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆå¤šä¸ªå›ç­”å»ºè®®ï¼Œä½¿ç”¨Server-Sent Eventså®ç°æµå¼å“åº”
    æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œè¿œç¨‹APIæœåŠ¡å•†
    """
    global llm_model, use_remote_llm
    
    logger.info("ğŸš€ å¼€å§‹å¤„ç†ç”Ÿæˆå»ºè®®è¯·æ±‚")
    logger.info(f"ğŸ“‹ è¯·æ±‚å‚æ•°: {request.dict()}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„LLMæœåŠ¡
    if not use_remote_llm and llm_model is None:
        logger.error("âŒ LLMæœåŠ¡æœªé…ç½®")
        raise HTTPException(status_code=503, detail="LLMæœåŠ¡æœªé…ç½®ï¼Œè¯·æ£€æŸ¥æœ¬åœ°æ¨¡å‹æˆ–è¿œç¨‹APIé…ç½®")
    
    try:
        # åŠ è½½ç³»ç»Ÿæç¤ºè¯
        logger.info("ğŸ“– åŠ è½½ç³»ç»Ÿæç¤ºè¯...")
        system_prompt = load_system_prompt()
        logger.info(f"ğŸ“– ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
        
        # æ„å»ºç»“æ„åŒ–çš„ç”¨æˆ·Prompt
        logger.info("ğŸ”§ æ„å»ºç”¨æˆ·æç¤ºè¯...")
        user_prompt = build_structured_prompt(request)
        logger.info(f"ğŸ”§ ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)} å­—ç¬¦")
        logger.debug(f"ğŸ“ ç”¨æˆ·æç¤ºè¯å†…å®¹: {user_prompt}")
        
        service_type = "è¿œç¨‹API" if use_remote_llm else "æœ¬åœ°æ¨¡å‹"
        logger.info(f"âš™ï¸ å¼€å§‹ç”Ÿæˆå»ºè®®ï¼Œä½¿ç”¨ {service_type}")
        
    except Exception as e:
        logger.error(f"âŒ è¯·æ±‚é¢„å¤„ç†å¤±è´¥: {type(e).__name__}: {str(e)}")
        logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"è¯·æ±‚é¢„å¤„ç†å¤±è´¥: {str(e)}")
    
    # ç”Ÿæˆå™¨å‡½æ•°ï¼Œç”¨äºæµå¼å“åº”
    async def generate_stream():
        try:
            start_time = time.time()
            accumulated_text = ""
            logger.info("ğŸ”„ å¼€å§‹ç”Ÿæˆæµå¼å“åº”...")
            
            if use_remote_llm:
                # ä½¿ç”¨è¿œç¨‹APIæœåŠ¡å•†
                logger.info("ğŸŒ ä½¿ç”¨è¿œç¨‹APIç”Ÿæˆå»ºè®®")
                
                try:
                    async for chunk in call_remote_llm_api(system_prompt, user_prompt):
                        if chunk["event"] == "token":
                            # ç›´æ¥è½¬å‘tokenäº‹ä»¶
                            yield chunk
                            
                            # æ›´æ–°ç´¯ç§¯æ–‡æœ¬
                            token_data = json.loads(chunk["data"])
                            accumulated_text = token_data["accumulated"]
                            
                            # æ·»åŠ å°å»¶è¿Ÿ
                            await asyncio.sleep(0.01)
                
                except Exception as e:
                    logger.error(f"âŒ è¿œç¨‹APIè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {str(e)}")
                    logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    raise
                
            else:
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                logger.info("ğŸ  ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆå»ºè®®")
                
                try:
                    # åº”ç”¨Qwen2èŠå¤©æ¨¡æ¿
                    logger.info("ğŸ“ åº”ç”¨Qwen2èŠå¤©æ¨¡æ¿...")
                    formatted_prompt = apply_qwen2_chat_template(system_prompt, user_prompt)
                    logger.info(f"ğŸ“ æ ¼å¼åŒ–åçš„æç¤ºè¯é•¿åº¦: {len(formatted_prompt)} å­—ç¬¦")
                    logger.debug(f"ğŸ“ æ ¼å¼åŒ–åçš„æç¤ºè¯: {formatted_prompt}")
                    
                    # ctransformersæµå¼ç”Ÿæˆ
                    logger.info("ğŸ”„ å¼€å§‹æœ¬åœ°æ¨¡å‹æµå¼ç”Ÿæˆ...")
                    token_count = 0
                    
                    for token in llm_model(
                        formatted_prompt,
                        max_new_tokens=1024,
                        temperature=0.7,
                        top_p=0.9,
                        stop=["<|im_end|>", "<|endoftext|>"],
                        stream=True,
                        reset=False  # ä¸é‡ç½®å¯¹è¯å†å²
                    ):
                        accumulated_text += token
                        token_count += 1
                        
                        # æ¯100ä¸ªtokenè®°å½•ä¸€æ¬¡è¿›åº¦
                        if token_count % 100 == 0:
                            logger.debug(f"ğŸ“Š æœ¬åœ°æ¨¡å‹å·²ç”Ÿæˆ {token_count} ä¸ªtokenï¼Œå½“å‰é•¿åº¦: {len(accumulated_text)}")
                        
                        # å‘é€tokenæ•°æ®
                        yield {
                            "event": "token",
                            "data": json.dumps({
                                "token": token,
                                "accumulated": accumulated_text
                            }, ensure_ascii=False)
                        }
                        
                        # æ·»åŠ å°å»¶è¿Ÿä»¥æ¨¡æ‹ŸçœŸå®çš„æµå¼æ•ˆæœ
                        await asyncio.sleep(0.01)
                    
                    logger.info(f"ğŸ“Š æœ¬åœ°æ¨¡å‹ç”Ÿæˆå®Œæˆï¼Œæ€»å…±ç”Ÿæˆ {token_count} ä¸ªtoken")
                    
                except Exception as e:
                    logger.error(f"âŒ æœ¬åœ°æ¨¡å‹è°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {str(e)}")
                    logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    raise
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            logger.info(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            logger.info(f"ğŸ“Š ç´¯ç§¯æ–‡æœ¬é•¿åº¦: {len(accumulated_text)} å­—ç¬¦")
            
            # è§£æJSONç»“æœ
            logger.info("ğŸ”„ å¼€å§‹è§£æJSONç»“æœ...")
            try:
                # å°è¯•è§£æJSON
                json_text = accumulated_text.strip()
                logger.debug(f"ğŸ“ åŸå§‹æ–‡æœ¬: {json_text[:200]}...")
                
                # å¦‚æœæ–‡æœ¬åŒ…å«markdownä»£ç å—ï¼Œæå–å…¶ä¸­çš„JSON
                if "```json" in json_text:
                    logger.info("ğŸ“ æ£€æµ‹åˆ°markdownä»£ç å—ï¼Œæå–JSON...")
                    start_idx = json_text.find("```json") + 7
                    end_idx = json_text.find("```", start_idx)
                    if end_idx > start_idx:
                        json_text = json_text[start_idx:end_idx].strip()
                        logger.info(f"ğŸ“ æå–çš„JSONé•¿åº¦: {len(json_text)} å­—ç¬¦")
                
                logger.info("ğŸ”„ å°è¯•è§£æJSON...")
                result = json.loads(json_text)
                logger.info(f"âœ… JSONè§£ææˆåŠŸ: {result}")
                
                # éªŒè¯JSONç»“æ„ç¬¦åˆSchema
                if 'suggestions' in result and isinstance(result['suggestions'], list):
                    suggestions = result['suggestions']
                    logger.info(f"âœ… æ‰¾åˆ° {len(suggestions)} ä¸ªå»ºè®®")
                    
                    # éªŒè¯æ¯ä¸ªå»ºè®®çš„ç»“æ„
                    for i, suggestion in enumerate(suggestions):
                        if not all(key in suggestion for key in ['id', 'content', 'confidence']):
                            logger.warning(f"âš ï¸ å»ºè®® {i+1} æ ¼å¼ä¸å®Œæ•´: {suggestion}")
                            raise ValueError(f"å»ºè®® {i+1} æ ¼å¼ä¸å®Œæ•´")
                        logger.debug(f"âœ… å»ºè®® {i+1} æ ¼å¼æ­£ç¡®")
                else:
                    logger.error("âŒ JSONç»“æ„ä¸ç¬¦åˆé¢„æœŸï¼Œç¼ºå°‘suggestionså­—æ®µæˆ–ç±»å‹é”™è¯¯")
                    raise ValueError("JSONç»“æ„ä¸ç¬¦åˆé¢„æœŸ")
                
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {type(e).__name__}: {str(e)}")
                logger.warning(f"âš ï¸ åŸå§‹æ–‡æœ¬å‰100å­—ç¬¦: {accumulated_text[:100]}...")
                logger.warning("âš ï¸ åˆ›å»ºåŒ…å«åŸå§‹æ–‡æœ¬çš„å“åº”")
                
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ›å»ºåŒ…å«åŸå§‹æ–‡æœ¬çš„å“åº”
                suggestions = [
                    {
                        "id": 1,
                        "content": accumulated_text[:200] + "..." if len(accumulated_text) > 200 else accumulated_text,
                        "confidence": 0.75
                    }
                ]
            
            # å‘é€æœ€ç»ˆç»“æœ
            logger.info("ğŸ“¤ å‘é€æœ€ç»ˆç»“æœ...")
            yield {
                "event": "complete",
                "data": json.dumps({
                    "suggestions": suggestions,
                    "processing_time": processing_time,
                    "raw_text": accumulated_text,
                    "service_type": "remote_api" if use_remote_llm else "local_model"
                }, ensure_ascii=False)
            }
            
            logger.info("âœ… ç”Ÿæˆå»ºè®®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ LLMç”Ÿæˆå¤±è´¥: {type(e).__name__}: {str(e)}")
            logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            yield {
                "event": "error",
                "data": json.dumps({
                    "error": f"ç”Ÿæˆå¤±è´¥: {str(e)}",
                    "error_type": type(e).__name__,
                    "service_type": "remote_api" if use_remote_llm else "local_model",
                    "traceback": traceback.format_exc()
                }, ensure_ascii=False)
            }
    
    # è¿”å›Server-Sent Eventså“åº”
    return EventSourceResponse(generate_stream())


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)